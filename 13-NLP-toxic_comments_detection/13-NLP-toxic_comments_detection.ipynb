{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>TF-IDF</a></span></li><li><span><a href=\"#BERT\" data-toc-modified-id=\"BERT-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>BERT</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Выводы</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Логистическая-регрессия\" data-toc-modified-id=\"Логистическая-регрессия-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Логистическая регрессия</a></span></li><li><span><a href=\"#Рандомный-лес\" data-toc-modified-id=\"Рандомный-лес-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Рандомный лес</a></span></li><li><span><a href=\"#Лучшая-модель\" data-toc-modified-id=\"Лучшая-модель-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Лучшая модель</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определение тональности комментариев (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "**Цель**\n",
    "\n",
    "Обучить модель, которая будет классифицировать комментарии на позитивные и негативные.\n",
    "\n",
    "**Задача**\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "В распоряжении набор данных с разметкой о токсичности правок. Столбец *text* содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (4.12.5)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers) (4.61.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (2022.8.17)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.9.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.9/site-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (1.21.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers) (8.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# подготовка данных TF-IDF\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# подготовка данных BERT\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from tqdm import notebook\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# модели МО\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# константы\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.20\n",
    "SAMPLES = 5000      # размер семплов для BERT\n",
    "MAX_LEN = 512       # максимальная длина токена для BERT\n",
    "BATCH_SIZE = 100    # размер батчей для ембеддингов BERT\n",
    "\n",
    "\n",
    "# снимаем ограничение на количество столбцов\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# снимаем ограничение на ширину столбцов\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# выставляем ограничение на показ знаков после запятой\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "# установка ядра tqdm_notebook для отображения прогресса в цикле\n",
    "notebook.tqdm_notebook.pandas()\n",
    "\n",
    "# скачиваем \n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(file_name: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Чтение csv-файла из этой же директории или с сервера яндекса\n",
    "    '''\n",
    "    local_pth = file_name\n",
    "    server_pth = 'https://example.ru/' + file_name\n",
    "\n",
    "    try:\n",
    "        data = pd.read_csv(local_pth, index_col='Unnamed: 0')\n",
    "    except:\n",
    "        data = pd.read_csv(server_pth, index_col='Unnamed: 0')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем файл\n",
    "df_comments = import_data('toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31055</th>\n",
       "      <td>Sometime back, I just happened to log on to www.izoom.in with a friend’s reference and I was amazed to see the concept Fresh Ideas Entertainment has come up with. So many deals… all under one roof. This website is very user friendly and easy to use and is fun to be on.\\nYou have Gossip, Games, Facts… Another exciting feature to add to it is Face of the Week… Every week, 4 new faces are selected and put up as izoom faces. It’s great to have been selected in four out of a group of millions. \\nThis new start up has already got many a deals in its kitty. Few of them being TheFortune Hotel, The Beach… are my personal favorites. izoom.in has a USP of mobile coupons. Coupons are available even when a user cannot access internet. You just need to SMS izoom support to 56767 and you get attended immediately.\\nAll I can say is izoom.in is a must visit website for everyone before they go out for shopping or dining or for outing.\\nCheers!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102929</th>\n",
       "      <td>\"\\n\\nThe latest edit is much better, don't make this article state \"\"super.\"\" at all. 71.237.70.49  \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67385</th>\n",
       "      <td>\" October 2007 (UTC)\\n\\nI would think you'd be able to get your point across, and be immune to any objections, were you to simply embellish the second sentence of the article by changing \"\"he was schooled at Thornleigh Salesian College\"\" to \"\"he was schooled at (the then all-Catholic) Thornleigh Salesian College\"\".   \\n\\nGood suggestion from an Anon - what do you think? Rgds, -  07:53, 5\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81167</th>\n",
       "      <td>Thanks for the tip on the currency translation.  Think it's all done now.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90182</th>\n",
       "      <td>I would argue that if content on the Con in comparison to the Arts Music is out of proportion, then it warrants further contribution to the article, not the removal of an indepth piece of content. Also, as I mentioned before, the Arts Music unit has a notable history comparable to that of the Con itself. Because of this, I would further argue that content on the Arts Music Unit is more relevant to this article than the information on the Newcastle Conservatorium.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>\"=Reliable sources===\\nCheating:\\n\"\"Barry Bonds:Cheater\"\" from CBS, yea I kinda think that is reliable. \\n\"\"Dear Barry Bonds, You are either an outright cheater or very stupid\"\" from the USA Today \\n\"\"Yes, Barry Bonds is a cheater. He is a cheater of the worst sort\"\" \\nLying:\\n\"\"It's clear, Barry Bonds' a liar\"\" New York Daily News, another pretty freakin' reliable source. \\n\"\"Barry Lamar Bonds is a bad man\"\" Baseball Digest \\n\"\"but Bonds is a liar, a cheater, a whiner and a bad influence on America's youth\"\"  Mark Barnes\\n\\n==\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125422</th>\n",
       "      <td>WTF=\\n\\nHow The Fuck Does This Person Merit A Page On Wikipedia.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149142</th>\n",
       "      <td>cajuns, acadians\\nCajuns, acadians, louisianans, they're so many different names for different americans of french descent because their culture is so rich and somewhat so different but so close at the same time. I'm an acadian but more importantly I'm a french american so I really don't see why there should be a difference. \\n\\nIf you say there should be two different list, it doesn't make sense. The people on the french-american list should be in one of wiki-invented list of cajuns or acadians. I understand there are some more recent french-americans who are only 1 or 2 generations-old americans but this distinction isn't made for italian-americans, german-americans. I'm surprised to see Albert Einstein a fairly recent immigrant in American history next to Katherine Heigl, a 10 to 12 generations american. \\\\\\n\\nThis is all race-based, biased because french-bashing don't stop at the bush government level.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89784</th>\n",
       "      <td>Hi - I dropped a pin in Google Maps at the ceremonial site near Chief Tayak's grave and recorded the lat/long.  Is this not permitted? It maps correctly in other sites, as well, so I believe they're the correct coordinates. I live nearby, so could also visit the site and get the coordinates from GPS.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64323</th>\n",
       "      <td>Re removal of accessdate= for urls books \\n\\nThis is from Template:Cite book\\naccessdate: Full date when url was accessed. Should be used when url field is used.\\nSomeone has claimed the exact opposite &amp; removed the accessdates</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
       "31055   Sometime back, I just happened to log on to www.izoom.in with a friend’s reference and I was amazed to see the concept Fresh Ideas Entertainment has come up with. So many deals… all under one roof. This website is very user friendly and easy to use and is fun to be on.\\nYou have Gossip, Games, Facts… Another exciting feature to add to it is Face of the Week… Every week, 4 new faces are selected and put up as izoom faces. It’s great to have been selected in four out of a group of millions. \\nThis new start up has already got many a deals in its kitty. Few of them being TheFortune Hotel, The Beach… are my personal favorites. izoom.in has a USP of mobile coupons. Coupons are available even when a user cannot access internet. You just need to SMS izoom support to 56767 and you get attended immediately.\\nAll I can say is izoom.in is a must visit website for everyone before they go out for shopping or dining or for outing.\\nCheers!!!   \n",
       "102929                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \"\\n\\nThe latest edit is much better, don't make this article state \"\"super.\"\" at all. 71.237.70.49  \"   \n",
       "67385                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \" October 2007 (UTC)\\n\\nI would think you'd be able to get your point across, and be immune to any objections, were you to simply embellish the second sentence of the article by changing \"\"he was schooled at Thornleigh Salesian College\"\" to \"\"he was schooled at (the then all-Catholic) Thornleigh Salesian College\"\".   \\n\\nGood suggestion from an Anon - what do you think? Rgds, -  07:53, 5\"   \n",
       "81167                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Thanks for the tip on the currency translation.  Think it's all done now.   \n",
       "90182                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             I would argue that if content on the Con in comparison to the Arts Music is out of proportion, then it warrants further contribution to the article, not the removal of an indepth piece of content. Also, as I mentioned before, the Arts Music unit has a notable history comparable to that of the Con itself. Because of this, I would further argue that content on the Arts Music Unit is more relevant to this article than the information on the Newcastle Conservatorium.   \n",
       "1860                                                                                                                                                                                                                                                                                                                                                                                                                           \"=Reliable sources===\\nCheating:\\n\"\"Barry Bonds:Cheater\"\" from CBS, yea I kinda think that is reliable. \\n\"\"Dear Barry Bonds, You are either an outright cheater or very stupid\"\" from the USA Today \\n\"\"Yes, Barry Bonds is a cheater. He is a cheater of the worst sort\"\" \\nLying:\\n\"\"It's clear, Barry Bonds' a liar\"\" New York Daily News, another pretty freakin' reliable source. \\n\"\"Barry Lamar Bonds is a bad man\"\" Baseball Digest \\n\"\"but Bonds is a liar, a cheater, a whiner and a bad influence on America's youth\"\"  Mark Barnes\\n\\n==\"   \n",
       "125422                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               WTF=\\n\\nHow The Fuck Does This Person Merit A Page On Wikipedia.   \n",
       "149142                        cajuns, acadians\\nCajuns, acadians, louisianans, they're so many different names for different americans of french descent because their culture is so rich and somewhat so different but so close at the same time. I'm an acadian but more importantly I'm a french american so I really don't see why there should be a difference. \\n\\nIf you say there should be two different list, it doesn't make sense. The people on the french-american list should be in one of wiki-invented list of cajuns or acadians. I understand there are some more recent french-americans who are only 1 or 2 generations-old americans but this distinction isn't made for italian-americans, german-americans. I'm surprised to see Albert Einstein a fairly recent immigrant in American history next to Katherine Heigl, a 10 to 12 generations american. \\\\\\n\\nThis is all race-based, biased because french-bashing don't stop at the bush government level.   \n",
       "89784                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Hi - I dropped a pin in Google Maps at the ceremonial site near Chief Tayak's grave and recorded the lat/long.  Is this not permitted? It maps correctly in other sites, as well, so I believe they're the correct coordinates. I live nearby, so could also visit the site and get the coordinates from GPS.   \n",
       "64323                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Re removal of accessdate= for urls books \\n\\nThis is from Template:Cite book\\naccessdate: Full date when url was accessed. Should be used when url field is used.\\nSomeone has claimed the exact opposite & removed the accessdates   \n",
       "\n",
       "        toxic  \n",
       "31055       0  \n",
       "102929      0  \n",
       "67385       0  \n",
       "81167       0  \n",
       "90182       0  \n",
       "1860        1  \n",
       "125422      1  \n",
       "149142      0  \n",
       "89784       0  \n",
       "64323       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# выводим рандомные строки\n",
    "display(df_comments.sample(10, random_state=RANDOM_STATE))\n",
    "print()\n",
    "# информация о таблице\n",
    "df_comments.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. В данных имеются английские комментарии, большую часть которых нужно очистить от лишних символов ('/n', знаки пунктуации, математические символы).\n",
    "2. Имеется 159292 комментария, пропусков данных не наблюдается.\n",
    "3. Данные правильно типизированы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество явных дубликатов: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Количество явных дубликатов: {df_comments.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение классов ЦП:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0   0.90\n",
       "1   0.10\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# выведем распределение классов\n",
    "print('Распределение классов ЦП:')\n",
    "df_comments.toxic.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92964</th>\n",
       "      <td>Dude, get a fucking life you queer.. get a real paying job instead of e-wiki policing around, you 50 year old piece of shite... and your mom said to make your bed.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120398</th>\n",
       "      <td>The only nonsense I will approve is nonsense committed by 1)chaka zulu warriors. 2)anglican gay priests 3)cocksuck ers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47519</th>\n",
       "      <td>Thou shalt be ravished in thy buttocks \\n\\nIt's sure to be hurtin!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>Groin \\n\\nYou sad fuck. I'm going to restore my edit, and you are going to leave it, or else I wil knee you in the groin yourself, prick. 91.108.208.101</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55660</th>\n",
       "      <td>Cause of Death \\n\\nFuck exicution, put down Murder was the mutha fuckin' case, and Shawtnegger was tha murdera!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97286</th>\n",
       "      <td>Getting REALLY annoyed\\nOK the list here is barely usable.  It sucks.  I don't think we should change it because the Arcs and other useful information supposedly don't fit here. BUT WHY CANT I PUT A SIMPLE LINK TO A WEBSITE THAT DOES HAVE THAT INFORMATION????  That website has all kinds of valuable information that is DIRECTLY RELATED TO THE ARTICLE.  Yet some WikiNazi thinks that this is his personal article and keeps making up some BS about how my link is a form of advertisement or some shit.  There are NO ADS.  It is another wiki site, and it is 100% ON TOPIC WITH USEFUL INFORMATION THAT EVERYONE KEEPS TRYING TO ADD TO THIS ARTICLE.  Since he won't let his PERSONAL wiki site be altered with USEFUL INFORMATION then let me keep the link.  I'm so sick of pedantic fools making wikipedia so useless and difficult to use.  I get that this page should reflect wiki-standards but that link I added is 100% relevant and is far more justified than plenty of external links I've seen on other pages.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148604</th>\n",
       "      <td>Harun Yahya/Adnan Oktar \\n\\nThis is quoted from the wikipedia article on Harun Yahya:\\n\\nEven though he often writes about science, he has never actually studied any science at a university level.  (, , )\\n\\nThere are also many disturbing data in that article: he claims to be a moral authority and an expert on Muslim ethics but has anal and oral sex with children, claiming that it is halal. If we are to keep his work in this article he needs to be demonstrated as having some standing as a Quranic scholar. Is he so received in the Muslim world? Is there any evidence for this?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33368</th>\n",
       "      <td>\":::: \"\"I will ignore but rest assured that, should you cause any further disruption to articles or talk pages, I will report you and ask that you be blocked longer than you were last time.\"\" \\nSeems your only response is violence? Am i right.... American self appointed dictator asshole? Whats the matter? Cant handle it when it gets tough? Sometimes the other one hits back and what do you do American fag? Run home to mother you fuck.  \\n\\n\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39960</th>\n",
       "      <td>Ima rude son of a bitch too because I have a period remember im gay...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35349</th>\n",
       "      <td>This guy is so dirty. He's awesome FUCK YEAH AHAHAHHAAHA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              text  \\\n",
       "92964                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Dude, get a fucking life you queer.. get a real paying job instead of e-wiki policing around, you 50 year old piece of shite... and your mom said to make your bed.   \n",
       "120398                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      The only nonsense I will approve is nonsense committed by 1)chaka zulu warriors. 2)anglican gay priests 3)cocksuck ers   \n",
       "47519                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Thou shalt be ravished in thy buttocks \\n\\nIt's sure to be hurtin!   \n",
       "815                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Groin \\n\\nYou sad fuck. I'm going to restore my edit, and you are going to leave it, or else I wil knee you in the groin yourself, prick. 91.108.208.101   \n",
       "55660                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Cause of Death \\n\\nFuck exicution, put down Murder was the mutha fuckin' case, and Shawtnegger was tha murdera!   \n",
       "97286   Getting REALLY annoyed\\nOK the list here is barely usable.  It sucks.  I don't think we should change it because the Arcs and other useful information supposedly don't fit here. BUT WHY CANT I PUT A SIMPLE LINK TO A WEBSITE THAT DOES HAVE THAT INFORMATION????  That website has all kinds of valuable information that is DIRECTLY RELATED TO THE ARTICLE.  Yet some WikiNazi thinks that this is his personal article and keeps making up some BS about how my link is a form of advertisement or some shit.  There are NO ADS.  It is another wiki site, and it is 100% ON TOPIC WITH USEFUL INFORMATION THAT EVERYONE KEEPS TRYING TO ADD TO THIS ARTICLE.  Since he won't let his PERSONAL wiki site be altered with USEFUL INFORMATION then let me keep the link.  I'm so sick of pedantic fools making wikipedia so useless and difficult to use.  I get that this page should reflect wiki-standards but that link I added is 100% relevant and is far more justified than plenty of external links I've seen on other pages.   \n",
       "148604                                                                                                                                                                                                                                                                                                                                                                                                                                       Harun Yahya/Adnan Oktar \\n\\nThis is quoted from the wikipedia article on Harun Yahya:\\n\\nEven though he often writes about science, he has never actually studied any science at a university level.  (, , )\\n\\nThere are also many disturbing data in that article: he claims to be a moral authority and an expert on Muslim ethics but has anal and oral sex with children, claiming that it is halal. If we are to keep his work in this article he needs to be demonstrated as having some standing as a Quranic scholar. Is he so received in the Muslim world? Is there any evidence for this?   \n",
       "33368                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \":::: \"\"I will ignore but rest assured that, should you cause any further disruption to articles or talk pages, I will report you and ask that you be blocked longer than you were last time.\"\" \\nSeems your only response is violence? Am i right.... American self appointed dictator asshole? Whats the matter? Cant handle it when it gets tough? Sometimes the other one hits back and what do you do American fag? Run home to mother you fuck.  \\n\\n\"   \n",
       "39960                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Ima rude son of a bitch too because I have a period remember im gay...   \n",
       "35349                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     This guy is so dirty. He's awesome FUCK YEAH AHAHAHHAAHA   \n",
       "\n",
       "        toxic  \n",
       "92964       1  \n",
       "120398      1  \n",
       "47519       1  \n",
       "815         1  \n",
       "55660       1  \n",
       "97286       1  \n",
       "148604      1  \n",
       "33368       1  \n",
       "39960       1  \n",
       "35349       1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# выведем токсичные комментарии\n",
    "df_comments.query('toxic==1').sample(10, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальная длинна комментария: 5000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Максимальная длинна комментария: {max(df_comments['text'].str.len())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для быстрого обучения моделей сократим выборку данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    50000 non-null  object\n",
      " 1   toxic   50000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# скопируем исходный файл\n",
    "data = df_comments.sample(random_state=RANDOM_STATE, n=SAMPLES * 10).reset_index(drop=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   0.90\n",
       "1   0.10\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# выведем распределение классов\n",
    "data.toxic.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    \"\"\"\n",
    "    Класс для обработки текста, включая очистку от лишних символов, удаление стоп-слов и лемматизацию.\n",
    "\n",
    "    Атрибуты:\n",
    "        stopwords (set): Множество стоп-слов для указанного языка.\n",
    "        lemmatizer (WordNetLemmatizer): Объект для лемматизации слов с использованием WordNetLemmatizer.\n",
    "\n",
    "    Методы:\n",
    "        clear_text(text): Очищает текст от лишних символов и стоп-слов.\n",
    "        lemm_text(text): Лемматизирует текст.\n",
    "        postag_lemm_text(text): Лемматизирует текст с учетом частей речи.\n",
    "        get_wordnet_pos(word): Возвращает POS-тег WordNet для слова.\n",
    "\n",
    "    \"\"\"\n",
    "    # Инициализация класса\n",
    "    def __init__(self, stopwords_language='english'):\n",
    "        self.stopwords = set(stopwords.words(stopwords_language)) # загрузка \n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        \n",
    "    # Метод для очистки текста от лишних символов и стоп-слов\n",
    "    def clear_text(self, text):\n",
    "        text = text.lower() \n",
    "        word_list = re.sub(r\"[^a-z ]\", ' ', text).split()\n",
    "        word_notstop_list = [w for w in word_list if w not in self.stopwords]\n",
    "        return ' '.join(word_notstop_list)\n",
    "    \n",
    "    # Метод для лемматизации текста\n",
    "    def lemm_text(self, text):\n",
    "        word_list = text.split()\n",
    "        lemmatized_text = ' '.join([self.wnl.lemmatize(w) for w in word_list])\n",
    "        return lemmatized_text\n",
    "    \n",
    "    # Метод для лемматизации текста с учетом частей речи\n",
    "    def postag_lemm_text(self, text):\n",
    "        word_list = text.split()\n",
    "        lemmatized_text = ' '.join([self.wnl.lemmatize(w, self.get_wordnet_pos(w)) for w in word_list])\n",
    "        return lemmatized_text\n",
    "    \n",
    "    @staticmethod\n",
    "    # Статический метод для определения части речи слова с использованием pos_tag\n",
    "    def get_wordnet_pos(word):\n",
    "        # Получение POS-тега для слова с использованием pos_tag\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        # Отображение POS-тегов WordNet на первую букву, используемую lemmatize\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        # Возврат соответствующего POS-тега WordNet или 'n' (существительное) по умолчанию\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Инициализация объекта для обработки текста\n",
    "text_processor = TextProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff15e3bd794245969663ee5428143021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6483c3e9e12345f7a8e2cfb3c9cd5813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# очистка текста\n",
    "data['clean_text'] = data['text'].progress_apply(text_processor.clear_text)\n",
    "\n",
    "# # лемматизация текста\n",
    "# data['wnl_text'] = data['clean_text'].progress_apply(text_processor.lemm_text)\n",
    "\n",
    "# лемматизация с POS-тегами\n",
    "data['wnlpostag_text'] = data['clean_text'].progress_apply(text_processor.postag_lemm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>wnlpostag_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"\\n\\nThe latest edit is much better, don't make this article state \"\"super.\"\" at all. 71.237.70.49  \"</td>\n",
       "      <td>0</td>\n",
       "      <td>latest edit much better make article state super</td>\n",
       "      <td>late edit much well make article state super</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I would argue that if content on the Con in comparison to the Arts Music is out of proportion, then it warrants further contribution to the article, not the removal of an indepth piece of content. Also, as I mentioned before, the Arts Music unit has a notable history comparable to that of the Con itself. Because of this, I would further argue that content on the Arts Music Unit is more relevant to this article than the information on the Newcastle Conservatorium.</td>\n",
       "      <td>0</td>\n",
       "      <td>would argue content con comparison arts music proportion warrants contribution article removal indepth piece content also mentioned arts music unit notable history comparable con would argue content arts music unit relevant article information newcastle conservatorium</td>\n",
       "      <td>would argue content con comparison art music proportion warrant contribution article removal indepth piece content also mention art music unit notable history comparable con would argue content art music unit relevant article information newcastle conservatorium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" October 2007 (UTC)\\n\\nI would think you'd be able to get your point across, and be immune to any objections, were you to simply embellish the second sentence of the article by changing \"\"he was schooled at Thornleigh Salesian College\"\" to \"\"he was schooled at (the then all-Catholic) Thornleigh Salesian College\"\".   \\n\\nGood suggestion from an Anon - what do you think? Rgds, -  07:53, 5\"</td>\n",
       "      <td>0</td>\n",
       "      <td>october utc would think able get point across immune objections simply embellish second sentence article changing schooled thornleigh salesian college schooled catholic thornleigh salesian college good suggestion anon think rgds</td>\n",
       "      <td>october utc would think able get point across immune objection simply embellish second sentence article change school thornleigh salesian college school catholic thornleigh salesian college good suggestion anon think rgds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sometime back, I just happened to log on to www.izoom.in with a friend’s reference and I was amazed to see the concept Fresh Ideas Entertainment has come up with. So many deals… all under one roof. This website is very user friendly and easy to use and is fun to be on.\\nYou have Gossip, Games, Facts… Another exciting feature to add to it is Face of the Week… Every week, 4 new faces are selected and put up as izoom faces. It’s great to have been selected in four out of a group of millions. \\nThis new start up has already got many a deals in its kitty. Few of them being TheFortune Hotel, The Beach… are my personal favorites. izoom.in has a USP of mobile coupons. Coupons are available even when a user cannot access internet. You just need to SMS izoom support to 56767 and you get attended immediately.\\nAll I can say is izoom.in is a must visit website for everyone before they go out for shopping or dining or for outing.\\nCheers!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>sometime back happened log www izoom friend reference amazed see concept fresh ideas entertainment come many deals one roof website user friendly easy use fun gossip games facts another exciting feature add face week every week new faces selected put izoom faces great selected four group millions new start already got many deals kitty thefortune hotel beach personal favorites izoom usp mobile coupons coupons available even user cannot access internet need sms izoom support get attended immediately say izoom must visit website everyone go shopping dining outing cheers</td>\n",
       "      <td>sometime back happen log www izoom friend reference amaze see concept fresh idea entertainment come many deal one roof website user friendly easy use fun gossip game fact another excite feature add face week every week new face select put izoom face great select four group million new start already get many deal kitty thefortune hotel beach personal favorite izoom usp mobile coupon coupon available even user cannot access internet need sm izoom support get attend immediately say izoom must visit website everyone go shopping din out cheer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thanks for the tip on the currency translation.  Think it's all done now.</td>\n",
       "      <td>0</td>\n",
       "      <td>thanks tip currency translation think done</td>\n",
       "      <td>thanks tip currency translation think do</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            text  \\\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \"\\n\\nThe latest edit is much better, don't make this article state \"\"super.\"\" at all. 71.237.70.49  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            I would argue that if content on the Con in comparison to the Arts Music is out of proportion, then it warrants further contribution to the article, not the removal of an indepth piece of content. Also, as I mentioned before, the Arts Music unit has a notable history comparable to that of the Con itself. Because of this, I would further argue that content on the Arts Music Unit is more relevant to this article than the information on the Newcastle Conservatorium.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \" October 2007 (UTC)\\n\\nI would think you'd be able to get your point across, and be immune to any objections, were you to simply embellish the second sentence of the article by changing \"\"he was schooled at Thornleigh Salesian College\"\" to \"\"he was schooled at (the then all-Catholic) Thornleigh Salesian College\"\".   \\n\\nGood suggestion from an Anon - what do you think? Rgds, -  07:53, 5\"   \n",
       "0  Sometime back, I just happened to log on to www.izoom.in with a friend’s reference and I was amazed to see the concept Fresh Ideas Entertainment has come up with. So many deals… all under one roof. This website is very user friendly and easy to use and is fun to be on.\\nYou have Gossip, Games, Facts… Another exciting feature to add to it is Face of the Week… Every week, 4 new faces are selected and put up as izoom faces. It’s great to have been selected in four out of a group of millions. \\nThis new start up has already got many a deals in its kitty. Few of them being TheFortune Hotel, The Beach… are my personal favorites. izoom.in has a USP of mobile coupons. Coupons are available even when a user cannot access internet. You just need to SMS izoom support to 56767 and you get attended immediately.\\nAll I can say is izoom.in is a must visit website for everyone before they go out for shopping or dining or for outing.\\nCheers!!!   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Thanks for the tip on the currency translation.  Think it's all done now.   \n",
       "\n",
       "   toxic  \\\n",
       "1      0   \n",
       "4      0   \n",
       "2      0   \n",
       "0      0   \n",
       "3      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      clean_text  \\\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               latest edit much better make article state super   \n",
       "4                                                                                                                                                                                                                                                                                                                   would argue content con comparison arts music proportion warrants contribution article removal indepth piece content also mentioned arts music unit notable history comparable con would argue content arts music unit relevant article information newcastle conservatorium   \n",
       "2                                                                                                                                                                                                                                                                                                                                                           october utc would think able get point across immune objections simply embellish second sentence article changing schooled thornleigh salesian college schooled catholic thornleigh salesian college good suggestion anon think rgds   \n",
       "0  sometime back happened log www izoom friend reference amazed see concept fresh ideas entertainment come many deals one roof website user friendly easy use fun gossip games facts another exciting feature add face week every week new faces selected put izoom faces great selected four group millions new start already got many deals kitty thefortune hotel beach personal favorites izoom usp mobile coupons coupons available even user cannot access internet need sms izoom support get attended immediately say izoom must visit website everyone go shopping dining outing cheers   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     thanks tip currency translation think done   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    wnlpostag_text  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     late edit much well make article state super  \n",
       "4                                                                                                                                                                                                                                                                                           would argue content con comparison art music proportion warrant contribution article removal indepth piece content also mention art music unit notable history comparable con would argue content art music unit relevant article information newcastle conservatorium  \n",
       "2                                                                                                                                                                                                                                                                                                                                    october utc would think able get point across immune objection simply embellish second sentence article change school thornleigh salesian college school catholic thornleigh salesian college good suggestion anon think rgds  \n",
       "0  sometime back happen log www izoom friend reference amaze see concept fresh idea entertainment come many deal one roof website user friendly easy use fun gossip game fact another excite feature add face week every week new face select put izoom face great select four group million new start already get many deal kitty thefortune hotel beach personal favorite izoom usp mobile coupon coupon available even user cannot access internet need sm izoom support get attend immediately say izoom must visit website everyone go shopping din out cheer  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         thanks tip currency translation think do  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на обучающую, валидационную и тренировочную выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разделение на тренировочный и тестовый наборы\n",
    "features, features_test, target, target_test = train_test_split(\n",
    "    data['wnlpostag_text'], data['toxic'], \n",
    "    stratify=data['toxic'],\n",
    "    test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# разделение данных на тренировочные и валидационные наборы\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    features, target,\n",
    "    stratify=target,\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно токенизировать данные, но это будет сделано в пайплайне, чтобы избежать утечек при кроссвалидации моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы с моделью BERT необходимо сократить выборку данных, так как токенизация занимает продолжительное время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    5000 non-null   object\n",
      " 1   toxic   5000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 78.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# сократим кол-во данных\n",
    "data_mini = df_comments.sample(random_state=RANDOM_STATE, n=SAMPLES).reset_index(drop=True)\n",
    "data_mini.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   0.90\n",
       "1   0.10\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# выведем распределение классов\n",
    "data_mini.toxic.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "**embeddings with cuda**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим ембеддинги с помощью предобученной модели `\"unitary/toxic-bert\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7551fbcd454d1da7bd80f205c20eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f48be9a74540f4989a681d98a2817d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at unitary/toxic-bert were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26614e6bb334685947c6a887ce6f687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/174 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d638759421ba406fbc136299611983b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa7d35bccac4a2f91205136e4b749e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"unitary/toxic-bert\"\n",
    "model = transformers.AutoModel.from_pretrained(model_name)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# токенизируем данные\n",
    "tokenized = data_mini['text'].apply(\n",
    "    lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=MAX_LEN, truncation=True)\n",
    ")\n",
    "\n",
    "# приводим строки к одной длине \n",
    "padded = np.array([i + [0]*(MAX_LEN - len(i)) for i in tokenized.values if len(i) <= MAX_LEN])\n",
    "\n",
    "# создаем маску\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# эмбеддинги по батчам\n",
    "embeddings = []\n",
    "for i in notebook.tqdm(range(padded.shape[0] // BATCH_SIZE)):\n",
    "    batch = torch.LongTensor(padded[BATCH_SIZE * i : BATCH_SIZE * (i + 1)])\n",
    "    attention_mask_batch = torch.LongTensor(attention_mask[BATCH_SIZE * i : BATCH_SIZE * (i + 1)])\n",
    "    batch = batch.to(device)\n",
    "    attention_mask_batch = attention_mask_batch.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "\n",
    "    embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# объединим нужные данные\n",
    "np_embeddings = np.concatenate(embeddings)\n",
    "\n",
    "# сохраним готовые ембеддинги в текущей директории\n",
    "np.save('toxic_comments_embeddings.npy', np_embeddings)\n",
    "\n",
    "np_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "**embeddings from local path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 768)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загрузка готовых эмбеддингов из директории\n",
    "np_embeddings = np.load('toxic_comments_embeddings.npy')\n",
    "np_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выборки для BERT\n",
    "features_bert, features_test_bert, target_bert, target_test_bert = train_test_split(\n",
    "    np_embeddings, data_mini['toxic'], \n",
    "    test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bert, X_valid_bert, y_train_bert, y_valid_bert = train_test_split(features_bert, target_bert, stratify=target_bert, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Загружены данные, пропусков и дубликатов данных нет.\n",
    "2. Подготовлены TF-IDF вектора и эмбеддинги.\n",
    "3. Данные разделены на обучающую, валидационную и тренировочную выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим датафрейм, в который будем записывать результаты подбора гиперпараметров\n",
    "models_results = pd.DataFrame(columns=['model_name', 'params', 'score', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gscv_fit(model, param_grid, X, y, model_name):\n",
    "    '''\n",
    "    Поиск гиперпараметров для модели с помощью GridSearchCV(), метрика F1\n",
    "    \n",
    "    Обновляет models_results\n",
    "    Возвращает \"best_estimator_\"\n",
    "    '''\n",
    "    \n",
    "    gscv = GridSearchCV(model, param_grid, scoring='f1', cv=5, n_jobs=-1, verbose=0)\n",
    "\n",
    "    gscv.fit(X, y)\n",
    "    start = time.time()\n",
    "    gscv.best_estimator_.fit(X, y)\n",
    "    end = time.time()\n",
    "    print(f'Best CV-F1 score Train: {gscv.best_score_:.2f}')\n",
    "    \n",
    "    global models_results\n",
    "    models_results = models_results.append({'model_name': model_name, 'params': gscv.best_params_, 'score': gscv.best_score_, 'time': end - start}, ignore_index=True)\n",
    "    \n",
    "    return gscv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# токениззатор для TF-IDF\n",
    "count_tf_idf = TfidfVectorizer()\n",
    "\n",
    "# модель логистической регрессии и гиперпараметры для нее\n",
    "model_logreg = LogisticRegression(class_weight='balanced', max_iter=500, random_state=RANDOM_STATE)\n",
    "param_grid_lr_tf_idf = {'model__C': [i/100 for i in range(1, 500, 50)]}\n",
    "param_grid_lr = {'C': [i/100 for i in range(1, 500, 50)]}\n",
    "\n",
    "# модель рандомного леса и гиперпараметры для нее\n",
    "model_forest =  RandomForestClassifier(class_weight='balanced', random_state=RANDOM_STATE)\n",
    "param_grid_frst_tf_idf = {\n",
    "    'model__n_estimators': range(10, 41, 10),\n",
    "    'model__max_depth': range(6, 16, 5)\n",
    "}\n",
    "param_grid_frst = {\n",
    "    'n_estimators': range(10, 41, 10),\n",
    "    'max_depth': range(6, 16, 5)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим линейные модели на разных тренировочных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV-F1 score Train: 0.75\n"
     ]
    }
   ],
   "source": [
    "log_reg_wnl = gscv_fit(\n",
    "    Pipeline([('tf_idf', count_tf_idf), ('model', model_logreg)]),\n",
    "    param_grid_lr_tf_idf,\n",
    "    X_train, y_train,\n",
    "    'LogReg_TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV-F1 score Train: 0.94\n"
     ]
    }
   ],
   "source": [
    "log_reg_bert = gscv_fit(model_logreg, param_grid_lr, X_train_bert, y_train_bert, 'LogReg_BERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рандомный лес"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим лес на разных тренировочных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV-F1 score Train: 0.36\n"
     ]
    }
   ],
   "source": [
    "forest_postag = gscv_fit(\n",
    "    Pipeline([('tf_idf', count_tf_idf), ('model', model_forest)]),\n",
    "    param_grid_frst_tf_idf,\n",
    "    X_train, y_train,\n",
    "    'RandForest_TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV-F1 score Train: 0.95\n"
     ]
    }
   ],
   "source": [
    "forest_bert = gscv_fit(model_forest, param_grid_frst, X_train_bert, y_train_bert, 'RandForest_BERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лучшая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>params</th>\n",
       "      <th>score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogReg_TF-IDF</td>\n",
       "      <td>{'model__C': 4.51}</td>\n",
       "      <td>0.75</td>\n",
       "      <td>15.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogReg_BERT</td>\n",
       "      <td>{'C': 0.51}</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandForest_TF-IDF</td>\n",
       "      <td>{'model__max_depth': 11, 'model__n_estimators': 40}</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandForest_BERT</td>\n",
       "      <td>{'max_depth': 6, 'n_estimators': 30}</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model_name                                               params  \\\n",
       "0      LogReg_TF-IDF                                   {'model__C': 4.51}   \n",
       "1        LogReg_BERT                                          {'C': 0.51}   \n",
       "2  RandForest_TF-IDF  {'model__max_depth': 11, 'model__n_estimators': 40}   \n",
       "3    RandForest_BERT                 {'max_depth': 6, 'n_estimators': 30}   \n",
       "\n",
       "   score  time  \n",
       "0   0.75 15.70  \n",
       "1   0.94  2.70  \n",
       "2   0.36  1.20  \n",
       "3   0.95  0.47  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# выведем результаты обучения моделей\n",
    "models_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказания моделей, обученных на ембеддингах BERT, точнее предсказаний моделей, обученных на TF-IDF векторах.\n",
    "\n",
    "Модель рандомного леса чуть лучше справилась с предсказанием на тренировочной выборке, чем модель логистической регрессии.\n",
    "\n",
    "Проверим модели на валидационных выборках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg F1 valid: 0.92\n",
      "RandForest F1 valid: 0.91\n"
     ]
    }
   ],
   "source": [
    "print(f'LogReg F1 valid: {f1_score(y_valid_bert, log_reg_bert.predict(X_valid_bert)):.2f}')\n",
    "print(f'RandForest F1 valid: {f1_score(y_valid_bert, forest_bert.predict(X_valid_bert)):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия лучше справилась с предсказаниями на валидационных данных, значит модель леса переобучилась на тренировочных данных. \n",
    "\n",
    "Дообучим модель леса. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрика F1 на тестовых данных: 0.96\n"
     ]
    }
   ],
   "source": [
    "# дообучим модель features_bert == X_train_bert + X_valid_bert\n",
    "forest_bert.fit(features_bert, target_bert)\n",
    "# предсказания модели на тренировочных данных\n",
    "preds = forest_bert.predict(features_test_bert)\n",
    "# выведем метрику\n",
    "print(f'Метрика F1 на тестовых данных: {f1_score(target_test_bert, preds):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика соответсвует требованиям к качаству предсказаний (< 0.75)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Были загружены данные, пропусков и дубликатов данных не обнаружено.\n",
    "2. Для быстрой работы выборки были сокращены до 50000 для tf-idf векторизации данных и до 5000 для ембеддингов bert.\n",
    "3. Обучено 2 типа моделей с подбором гиперпараметров: логистическая регрессия, рандомный лес. Для подготовки обучающих выборок использовали 2 типа подготовки текста: TF-IDF векторизация с учетом частей речи, ембеддинги BERT.\n",
    "4. Модели, обученные на ембеддингах BERT, лучше детектируют токсичные комментарии. Модель рандомного леса справилась с тренироовочной выборкой лучше всех остальных. Для уменьшения влияния переобучения на тренировочных данных модель была дообучена.\n",
    "5. Метрика F1 лучшей модели на тестовой выборке составила 0.96."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1626,
    "start_time": "2024-12-24T06:25:08.585Z"
   },
   {
    "duration": 1799,
    "start_time": "2024-12-24T06:25:18.639Z"
   },
   {
    "duration": 3684,
    "start_time": "2024-12-24T06:26:00.413Z"
   },
   {
    "duration": 53,
    "start_time": "2024-12-24T06:26:24.561Z"
   },
   {
    "duration": 57,
    "start_time": "2024-12-24T06:26:29.170Z"
   },
   {
    "duration": 5,
    "start_time": "2024-12-24T06:26:32.602Z"
   },
   {
    "duration": 263,
    "start_time": "2024-12-24T06:26:56.144Z"
   },
   {
    "duration": 15,
    "start_time": "2024-12-24T06:27:01.469Z"
   },
   {
    "duration": 19,
    "start_time": "2024-12-24T06:27:15.813Z"
   },
   {
    "duration": 6,
    "start_time": "2024-12-24T06:27:38.161Z"
   },
   {
    "duration": 16,
    "start_time": "2024-12-24T06:27:55.570Z"
   },
   {
    "duration": 2190,
    "start_time": "2024-12-24T06:30:30.306Z"
   },
   {
    "duration": 15,
    "start_time": "2024-12-24T06:30:53.785Z"
   },
   {
    "duration": 2097,
    "start_time": "2024-12-24T06:31:26.851Z"
   },
   {
    "duration": 3701,
    "start_time": "2024-12-24T06:31:28.953Z"
   },
   {
    "duration": 2113,
    "start_time": "2024-12-24T06:31:32.655Z"
   },
   {
    "duration": 0,
    "start_time": "2024-12-24T06:31:34.769Z"
   },
   {
    "duration": 0,
    "start_time": "2024-12-24T06:31:34.770Z"
   },
   {
    "duration": 0,
    "start_time": "2024-12-24T06:31:34.771Z"
   },
   {
    "duration": 832,
    "start_time": "2024-12-24T06:32:13.944Z"
   },
   {
    "duration": 2636,
    "start_time": "2025-01-09T11:25:21.016Z"
   },
   {
    "duration": 1004,
    "start_time": "2025-01-09T11:25:27.214Z"
   },
   {
    "duration": 1548,
    "start_time": "2025-01-09T11:25:30.392Z"
   },
   {
    "duration": 624,
    "start_time": "2025-01-09T11:25:33.646Z"
   },
   {
    "duration": 4,
    "start_time": "2025-01-09T11:25:42.289Z"
   },
   {
    "duration": 2329,
    "start_time": "2025-01-09T11:25:45.895Z"
   },
   {
    "duration": 43,
    "start_time": "2025-01-09T11:25:48.226Z"
   },
   {
    "duration": 200,
    "start_time": "2025-01-09T11:25:51.664Z"
   },
   {
    "duration": 8,
    "start_time": "2025-01-09T11:25:53.793Z"
   },
   {
    "duration": 12,
    "start_time": "2025-01-09T11:25:57.247Z"
   },
   {
    "duration": 120,
    "start_time": "2025-01-09T11:26:00.026Z"
   },
   {
    "duration": 24,
    "start_time": "2025-01-09T11:26:02.291Z"
   },
   {
    "duration": 5,
    "start_time": "2025-01-09T11:26:03.539Z"
   },
   {
    "duration": 7,
    "start_time": "2025-01-09T11:26:06.036Z"
   },
   {
    "duration": 1824,
    "start_time": "2025-01-09T11:26:09.526Z"
   },
   {
    "duration": 6,
    "start_time": "2025-01-09T11:26:32.996Z"
   },
   {
    "duration": 8335,
    "start_time": "2025-01-09T11:26:37.430Z"
   },
   {
    "duration": 4,
    "start_time": "2025-01-09T11:28:51.325Z"
   },
   {
    "duration": 7,
    "start_time": "2025-01-09T11:28:56.113Z"
   },
   {
    "duration": 212258,
    "start_time": "2025-01-09T11:28:58.309Z"
   },
   {
    "duration": 26,
    "start_time": "2025-01-09T11:32:33.222Z"
   },
   {
    "duration": 4,
    "start_time": "2025-01-09T11:35:14.165Z"
   },
   {
    "duration": 2357,
    "start_time": "2025-01-09T11:36:39.757Z"
   },
   {
    "duration": 2159,
    "start_time": "2025-01-09T11:37:07.135Z"
   },
   {
    "duration": 75,
    "start_time": "2025-01-09T11:37:28.571Z"
   },
   {
    "duration": 77,
    "start_time": "2025-01-09T11:37:35.633Z"
   },
   {
    "duration": 17,
    "start_time": "2025-01-09T11:37:45.006Z"
   },
   {
    "duration": 92,
    "start_time": "2025-01-09T11:39:08.159Z"
   },
   {
    "duration": 7658,
    "start_time": "2025-01-09T11:39:12.185Z"
   },
   {
    "duration": 94,
    "start_time": "2025-01-09T11:39:30.259Z"
   },
   {
    "duration": 492,
    "start_time": "2025-01-09T11:39:57.180Z"
   },
   {
    "duration": 98,
    "start_time": "2025-01-09T11:42:19.421Z"
   },
   {
    "duration": 424,
    "start_time": "2025-01-09T11:42:45.629Z"
   },
   {
    "duration": 403,
    "start_time": "2025-01-09T11:42:51.260Z"
   },
   {
    "duration": 48,
    "start_time": "2025-01-09T11:43:50.019Z"
   },
   {
    "duration": 1344,
    "start_time": "2025-01-09T11:43:51.650Z"
   },
   {
    "duration": 23,
    "start_time": "2025-01-09T11:43:59.846Z"
   },
   {
    "duration": 6,
    "start_time": "2025-01-09T11:44:00.643Z"
   },
   {
    "duration": 139,
    "start_time": "2025-01-09T11:44:32.991Z"
   },
   {
    "duration": 11,
    "start_time": "2025-01-09T11:44:35.618Z"
   },
   {
    "duration": 11,
    "start_time": "2025-01-09T11:44:36.371Z"
   },
   {
    "duration": 5,
    "start_time": "2025-01-09T11:44:39.980Z"
   },
   {
    "duration": 4,
    "start_time": "2025-01-09T11:44:40.789Z"
   },
   {
    "duration": 4,
    "start_time": "2025-01-09T11:44:42.886Z"
   },
   {
    "duration": 569518,
    "start_time": "2025-01-09T11:44:59.713Z"
   },
   {
    "duration": 1109,
    "start_time": "2025-01-09T12:06:41.865Z"
   },
   {
    "duration": 1744,
    "start_time": "2025-01-09T12:06:44.515Z"
   },
   {
    "duration": 196,
    "start_time": "2025-01-09T12:06:46.261Z"
   },
   {
    "duration": 4,
    "start_time": "2025-01-09T12:06:48.878Z"
   },
   {
    "duration": 2653,
    "start_time": "2025-01-09T12:06:50.983Z"
   },
   {
    "duration": 38,
    "start_time": "2025-01-09T12:06:53.637Z"
   },
   {
    "duration": 210,
    "start_time": "2025-01-09T12:06:53.688Z"
   },
   {
    "duration": 8,
    "start_time": "2025-01-09T12:06:54.062Z"
   },
   {
    "duration": 12,
    "start_time": "2025-01-09T12:06:54.770Z"
   },
   {
    "duration": 107,
    "start_time": "2025-01-09T12:06:55.366Z"
   },
   {
    "duration": 24,
    "start_time": "2025-01-09T12:06:56.694Z"
   },
   {
    "duration": 5,
    "start_time": "2025-01-09T12:06:56.937Z"
   },
   {
    "duration": 8,
    "start_time": "2025-01-09T12:06:57.191Z"
   },
   {
    "duration": 216102,
    "start_time": "2025-01-09T12:06:58.060Z"
   },
   {
    "duration": 25,
    "start_time": "2025-01-09T12:10:34.164Z"
   },
   {
    "duration": 54,
    "start_time": "2025-01-09T12:10:34.190Z"
   },
   {
    "duration": 14,
    "start_time": "2025-01-09T12:10:34.246Z"
   },
   {
    "duration": 12,
    "start_time": "2025-01-09T12:10:34.261Z"
   },
   {
    "duration": 254,
    "start_time": "2025-01-09T12:10:34.274Z"
   },
   {
    "duration": 11,
    "start_time": "2025-01-09T12:10:34.530Z"
   },
   {
    "duration": 28,
    "start_time": "2025-01-09T12:10:34.543Z"
   },
   {
    "duration": 12,
    "start_time": "2025-01-09T12:10:34.574Z"
   },
   {
    "duration": 14,
    "start_time": "2025-01-09T12:10:34.588Z"
   },
   {
    "duration": 27,
    "start_time": "2025-01-09T12:10:34.603Z"
   },
   {
    "duration": 17,
    "start_time": "2025-01-09T12:10:34.631Z"
   },
   {
    "duration": 7,
    "start_time": "2025-01-09T12:10:47.550Z"
   },
   {
    "duration": 1046,
    "start_time": "2025-01-09T12:11:50.866Z"
   },
   {
    "duration": 1375,
    "start_time": "2025-01-09T12:11:51.914Z"
   },
   {
    "duration": 309,
    "start_time": "2025-01-09T12:11:53.291Z"
   },
   {
    "duration": 4,
    "start_time": "2025-01-09T12:11:54.921Z"
   },
   {
    "duration": 2380,
    "start_time": "2025-01-09T12:11:55.696Z"
   },
   {
    "duration": 48,
    "start_time": "2025-01-09T12:11:58.079Z"
   },
   {
    "duration": 227,
    "start_time": "2025-01-09T12:11:58.129Z"
   },
   {
    "duration": 9,
    "start_time": "2025-01-09T12:11:58.358Z"
   },
   {
    "duration": 37,
    "start_time": "2025-01-09T12:11:58.368Z"
   },
   {
    "duration": 121,
    "start_time": "2025-01-09T12:11:58.406Z"
   },
   {
    "duration": 24,
    "start_time": "2025-01-09T12:11:58.529Z"
   },
   {
    "duration": 6,
    "start_time": "2025-01-09T12:11:58.682Z"
   },
   {
    "duration": 8,
    "start_time": "2025-01-09T12:11:58.992Z"
   },
   {
    "duration": 208646,
    "start_time": "2025-01-09T12:12:01.465Z"
   },
   {
    "duration": 28,
    "start_time": "2025-01-09T12:15:30.112Z"
   },
   {
    "duration": 58,
    "start_time": "2025-01-09T12:15:30.141Z"
   },
   {
    "duration": 3,
    "start_time": "2025-01-09T12:15:30.201Z"
   },
   {
    "duration": 109,
    "start_time": "2025-01-09T12:15:30.205Z"
   },
   {
    "duration": 33,
    "start_time": "2025-01-09T12:15:30.315Z"
   },
   {
    "duration": 245,
    "start_time": "2025-01-09T12:15:30.350Z"
   },
   {
    "duration": 10,
    "start_time": "2025-01-09T12:15:30.597Z"
   },
   {
    "duration": 34,
    "start_time": "2025-01-09T12:15:30.609Z"
   },
   {
    "duration": 14,
    "start_time": "2025-01-09T12:15:30.645Z"
   },
   {
    "duration": 25,
    "start_time": "2025-01-09T12:15:30.661Z"
   },
   {
    "duration": 30,
    "start_time": "2025-01-09T12:15:30.687Z"
   },
   {
    "duration": 3,
    "start_time": "2025-01-09T12:53:04.381Z"
   },
   {
    "duration": 3,
    "start_time": "2025-01-09T12:53:41.126Z"
   },
   {
    "duration": 257545,
    "start_time": "2025-01-09T12:53:58.198Z"
   },
   {
    "duration": 2,
    "start_time": "2025-01-09T12:58:31.216Z"
   },
   {
    "duration": 4,
    "start_time": "2025-01-09T12:58:48.824Z"
   },
   {
    "duration": 3,
    "start_time": "2025-01-09T12:58:50.626Z"
   },
   {
    "duration": 3,
    "start_time": "2025-01-09T12:58:53.831Z"
   },
   {
    "duration": 168,
    "start_time": "2025-01-09T12:58:56.743Z"
   },
   {
    "duration": 3,
    "start_time": "2025-01-09T13:06:08.522Z"
   },
   {
    "duration": 3,
    "start_time": "2025-01-09T13:06:17.422Z"
   },
   {
    "duration": 545018,
    "start_time": "2025-01-09T13:06:22.207Z"
   },
   {
    "duration": 28,
    "start_time": "2025-01-09T13:22:05.937Z"
   },
   {
    "duration": 4,
    "start_time": "2025-01-09T13:22:20.207Z"
   },
   {
    "duration": 106897,
    "start_time": "2025-01-09T13:22:23.454Z"
   },
   {
    "duration": 1261,
    "start_time": "2025-01-09T13:24:23.973Z"
   },
   {
    "duration": 2906,
    "start_time": "2025-01-09T13:24:25.235Z"
   },
   {
    "duration": 263,
    "start_time": "2025-01-09T13:24:28.142Z"
   },
   {
    "duration": 4,
    "start_time": "2025-01-09T13:24:28.406Z"
   },
   {
    "duration": 3851,
    "start_time": "2025-01-09T13:24:28.942Z"
   },
   {
    "duration": 48,
    "start_time": "2025-01-09T13:24:32.795Z"
   },
   {
    "duration": 221,
    "start_time": "2025-01-09T13:24:32.845Z"
   },
   {
    "duration": 8,
    "start_time": "2025-01-09T13:24:33.068Z"
   },
   {
    "duration": 45,
    "start_time": "2025-01-09T13:24:33.077Z"
   },
   {
    "duration": 122,
    "start_time": "2025-01-09T13:24:33.836Z"
   },
   {
    "duration": 26,
    "start_time": "2025-01-09T13:24:35.499Z"
   },
   {
    "duration": 7,
    "start_time": "2025-01-09T13:24:35.984Z"
   },
   {
    "duration": 8,
    "start_time": "2025-01-09T13:24:36.799Z"
   },
   {
    "duration": 1359,
    "start_time": "2025-01-09T13:24:53.756Z"
   },
   {
    "duration": 7,
    "start_time": "2025-01-09T13:25:02.879Z"
   },
   {
    "duration": 215222,
    "start_time": "2025-01-09T13:25:05.524Z"
   },
   {
    "duration": 26,
    "start_time": "2025-01-09T13:28:40.747Z"
   },
   {
    "duration": 47,
    "start_time": "2025-01-09T13:28:40.774Z"
   },
   {
    "duration": 14,
    "start_time": "2025-01-09T13:28:40.823Z"
   },
   {
    "duration": 20,
    "start_time": "2025-01-09T13:28:40.839Z"
   },
   {
    "duration": 276,
    "start_time": "2025-01-09T13:28:40.861Z"
   },
   {
    "duration": 10,
    "start_time": "2025-01-09T13:28:41.139Z"
   },
   {
    "duration": 14,
    "start_time": "2025-01-09T13:28:41.150Z"
   },
   {
    "duration": 4,
    "start_time": "2025-01-09T13:28:41.166Z"
   },
   {
    "duration": 5,
    "start_time": "2025-01-09T13:28:41.172Z"
   },
   {
    "duration": 6,
    "start_time": "2025-01-09T13:28:41.179Z"
   },
   {
    "duration": 50,
    "start_time": "2025-01-09T13:28:41.187Z"
   },
   {
    "duration": 0,
    "start_time": "2025-01-09T13:28:41.239Z"
   },
   {
    "duration": 0,
    "start_time": "2025-01-09T13:28:41.240Z"
   },
   {
    "duration": 0,
    "start_time": "2025-01-09T13:28:41.241Z"
   },
   {
    "duration": 0,
    "start_time": "2025-01-09T13:28:41.243Z"
   },
   {
    "duration": 0,
    "start_time": "2025-01-09T13:28:41.244Z"
   },
   {
    "duration": 0,
    "start_time": "2025-01-09T13:28:41.245Z"
   },
   {
    "duration": 5,
    "start_time": "2025-01-09T13:31:13.762Z"
   },
   {
    "duration": 32,
    "start_time": "2025-01-09T13:31:19.259Z"
   },
   {
    "duration": 168643,
    "start_time": "2025-01-09T13:31:47.385Z"
   },
   {
    "duration": 14360,
    "start_time": "2025-01-09T13:34:36.031Z"
   },
   {
    "duration": 5,
    "start_time": "2025-01-09T13:49:21.535Z"
   },
   {
    "duration": 505779,
    "start_time": "2025-01-09T13:49:25.153Z"
   },
   {
    "duration": 97,
    "start_time": "2025-01-09T13:57:50.934Z"
   },
   {
    "duration": 0,
    "start_time": "2025-01-09T13:57:51.033Z"
   },
   {
    "duration": 0,
    "start_time": "2025-01-09T13:57:51.034Z"
   },
   {
    "duration": 0,
    "start_time": "2025-01-09T13:57:51.035Z"
   },
   {
    "duration": 11,
    "start_time": "2025-01-09T14:00:59.538Z"
   },
   {
    "duration": 11,
    "start_time": "2025-01-09T14:01:15.099Z"
   },
   {
    "duration": 5,
    "start_time": "2025-01-09T14:01:17.674Z"
   },
   {
    "duration": 12,
    "start_time": "2025-01-09T14:01:20.719Z"
   },
   {
    "duration": 30,
    "start_time": "2025-01-09T14:01:26.837Z"
   },
   {
    "duration": 32,
    "start_time": "2025-01-09T14:01:50.437Z"
   },
   {
    "duration": 41863,
    "start_time": "2025-01-09T14:02:07.478Z"
   },
   {
    "duration": 8,
    "start_time": "2025-01-09T14:03:51.606Z"
   },
   {
    "duration": 47,
    "start_time": "2025-01-09T14:04:02.975Z"
   },
   {
    "duration": 724,
    "start_time": "2025-01-09T14:04:06.886Z"
   },
   {
    "duration": 1054,
    "start_time": "2025-01-09T14:04:30.062Z"
   },
   {
    "duration": 1384,
    "start_time": "2025-01-09T14:04:31.121Z"
   },
   {
    "duration": 280,
    "start_time": "2025-01-09T14:04:32.507Z"
   },
   {
    "duration": 3,
    "start_time": "2025-01-09T14:04:32.789Z"
   },
   {
    "duration": 2716,
    "start_time": "2025-01-09T14:04:33.775Z"
   },
   {
    "duration": 43,
    "start_time": "2025-01-09T14:04:36.493Z"
   },
   {
    "duration": 218,
    "start_time": "2025-01-09T14:04:36.538Z"
   },
   {
    "duration": 7,
    "start_time": "2025-01-09T14:04:36.758Z"
   },
   {
    "duration": 29,
    "start_time": "2025-01-09T14:04:36.766Z"
   },
   {
    "duration": 131,
    "start_time": "2025-01-09T14:04:36.796Z"
   },
   {
    "duration": 25,
    "start_time": "2025-01-09T14:04:36.928Z"
   },
   {
    "duration": 11,
    "start_time": "2025-01-09T14:04:36.954Z"
   },
   {
    "duration": 34,
    "start_time": "2025-01-09T14:04:36.966Z"
   },
   {
    "duration": 213221,
    "start_time": "2025-01-09T14:04:37.513Z"
   },
   {
    "duration": 25,
    "start_time": "2025-01-09T14:08:10.736Z"
   },
   {
    "duration": 33,
    "start_time": "2025-01-09T14:08:10.762Z"
   },
   {
    "duration": 35,
    "start_time": "2025-01-09T14:08:10.797Z"
   },
   {
    "duration": 90,
    "start_time": "2025-01-09T14:08:10.834Z"
   },
   {
    "duration": 16859,
    "start_time": "2025-01-09T14:08:10.926Z"
   },
   {
    "duration": 294,
    "start_time": "2025-01-09T14:08:27.787Z"
   },
   {
    "duration": 7,
    "start_time": "2025-01-09T14:08:28.083Z"
   },
   {
    "duration": 8,
    "start_time": "2025-01-09T14:08:28.092Z"
   },
   {
    "duration": 15,
    "start_time": "2025-01-09T14:08:28.102Z"
   },
   {
    "duration": 21,
    "start_time": "2025-01-09T14:08:28.118Z"
   },
   {
    "duration": 10,
    "start_time": "2025-01-09T14:08:28.140Z"
   },
   {
    "duration": 502273,
    "start_time": "2025-01-09T14:08:28.152Z"
   },
   {
    "duration": 165997,
    "start_time": "2025-01-09T14:16:50.427Z"
   },
   {
    "duration": 41748,
    "start_time": "2025-01-09T14:19:36.426Z"
   },
   {
    "duration": 14273,
    "start_time": "2025-01-09T14:20:18.175Z"
   },
   {
    "duration": 9,
    "start_time": "2025-01-09T14:20:32.450Z"
   },
   {
    "duration": 66,
    "start_time": "2025-01-09T14:20:32.461Z"
   },
   {
    "duration": 880,
    "start_time": "2025-01-09T14:20:32.529Z"
   },
   {
    "duration": 3140,
    "start_time": "2025-01-09T16:25:22.873Z"
   },
   {
    "duration": 2817,
    "start_time": "2025-01-09T16:25:26.016Z"
   },
   {
    "duration": 11175,
    "start_time": "2025-01-09T16:25:28.834Z"
   },
   {
    "duration": 688,
    "start_time": "2025-01-09T16:25:40.011Z"
   },
   {
    "duration": 4,
    "start_time": "2025-01-09T16:25:40.702Z"
   },
   {
    "duration": 2648,
    "start_time": "2025-01-09T16:25:40.708Z"
   },
   {
    "duration": 52,
    "start_time": "2025-01-09T16:25:43.365Z"
   },
   {
    "duration": 272,
    "start_time": "2025-01-09T16:25:43.419Z"
   },
   {
    "duration": 10,
    "start_time": "2025-01-09T16:25:43.692Z"
   },
   {
    "duration": 15,
    "start_time": "2025-01-09T16:25:43.704Z"
   },
   {
    "duration": 182,
    "start_time": "2025-01-09T16:25:43.721Z"
   },
   {
    "duration": 36,
    "start_time": "2025-01-09T16:25:43.905Z"
   },
   {
    "duration": 26,
    "start_time": "2025-01-09T16:25:43.943Z"
   },
   {
    "duration": 22,
    "start_time": "2025-01-09T16:25:43.974Z"
   },
   {
    "duration": 27,
    "start_time": "2025-01-09T16:25:43.998Z"
   },
   {
    "duration": 1644,
    "start_time": "2025-01-09T16:25:44.027Z"
   },
   {
    "duration": 14,
    "start_time": "2025-01-09T16:25:45.674Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "246.333px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
